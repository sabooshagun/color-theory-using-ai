{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Color Theory Analysis using AI**\n",
        "Color theory is a crucial aspect of design, affecting how we interpret aesthetics, convey messages, and evoke emotions. It encompasses the study of color relationships and the effects of color combinations in visual compositions. Traditional color theory analysis involves understanding color wheels, primary and secondary colors, complementary colors, and the impact of different hues and saturations on viewer perception.\n",
        "\n",
        "Artificial Intelligence (AI) has the potential to significantly ease and enhance the process of color theory analysis. AI can automate and optimize color selection, helping designers by suggesting harmonious color palettes based on desired emotions or themes. Additionally, AI-driven tools can analyze large datasets of images and designs to identify trending colors and patterns, providing insights that are not immediately apparent to the human eye.\n",
        "\n",
        "AI can also facilitate personalized color recommendations, adapting to individual preferences or specific brand guidelines. This customization extends to accessibility considerations, where AI can ensure that color palettes are perceivable by people with color vision deficiencies, enhancing the universal usability of designs.\n",
        "\n"
      ],
      "metadata": {
        "id": "kEhskyneecQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "File upload"
      ],
      "metadata": {
        "id": "qCJl5BBrgsk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "QxCMuwTFuCxJ",
        "outputId": "425bce41-9521-4243-ba3b-783dc520d50f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-29788fd4-0ffb-4cb8-b79f-eb1d8cf747a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-29788fd4-0ffb-4cb8-b79f-eb1d8cf747a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving shape_predictor_68_face_landmarks.dat to shape_predictor_68_face_landmarks.dat\n",
            "User uploaded file \"shape_predictor_68_face_landmarks.dat\" with length 99693937 bytes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries to install"
      ],
      "metadata": {
        "id": "BZFrOFfEgwQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python dlib numpy openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1qguDesuvUp",
        "outputId": "03c08ebd-883a-4b46-b707-0d29ddc9c143"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have divided the code in two parts first, to analyse the face and get face colors and secondly integrating it with open ai and running it together to generate analysis report"
      ],
      "metadata": {
        "id": "pbODYgX8hk9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the face detector and shape predictor models\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "#function  to get the color code by averaging pixel values.\n",
        "def get_average_color(image, region):\n",
        "    mask = np.zeros_like(image)\n",
        "    cv2.fillPoly(mask, [region], (255, 255, 255))\n",
        "    mean_color = cv2.mean(image, mask=mask[:, :, 0])\n",
        "    return mean_color[:3]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "collapsed": true,
        "id": "20lm8MutvJVp",
        "outputId": "dac08182-f00f-4619-8285-6c22c87fd607"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unable to open shape_predictor_68_face_landmarks.dat",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ce7a1747953e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the face detector and shape predictor models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code maps the face and calculate the the average of skin color and gives us the result in BGR Value.\n",
        "If you are using this code on colab, please make sure you use cv2_imshow else you can use cv2.imshow() as well."
      ],
      "metadata": {
        "id": "9NDtmESkiopL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image\n",
        "image = cv2.imread(\"image.jpeg\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces\n",
        "faces = detector(gray)\n",
        "for face in faces:\n",
        "    landmarks = predictor(gray, face)\n",
        "\n",
        "    # Extract regions based on landmarks\n",
        "    forehead = np.array([(landmarks.part(19).x, landmarks.part(19).y),\n",
        "                         (landmarks.part(24).x, landmarks.part(24).y),\n",
        "                         (landmarks.part(24).x, landmarks.part(24).y - 50),\n",
        "                         (landmarks.part(19).x, landmarks.part(19).y - 50)])\n",
        "\n",
        "    nose = np.array([(landmarks.part(27).x, landmarks.part(27).y),\n",
        "                     (landmarks.part(33).x, landmarks.part(33).y),\n",
        "                     (landmarks.part(31).x, landmarks.part(31).y),\n",
        "                     (landmarks.part(35).x, landmarks.part(35).y)])\n",
        "\n",
        "    left_cheek = np.array([(landmarks.part(1).x, landmarks.part(1).y),\n",
        "                           (landmarks.part(3).x, landmarks.part(3).y),\n",
        "                           (landmarks.part(31).x, landmarks.part(31).y),\n",
        "                           (landmarks.part(48).x, landmarks.part(48).y),\n",
        "                           (landmarks.part(36).x, landmarks.part(36).y)])\n",
        "\n",
        "    right_cheek = np.array([(landmarks.part(15).x, landmarks.part(15).y),\n",
        "                            (landmarks.part(13).x, landmarks.part(13).y),\n",
        "                            (landmarks.part(35).x, landmarks.part(35).y),\n",
        "                            (landmarks.part(54).x, landmarks.part(54).y),\n",
        "                            (landmarks.part(45).x, landmarks.part(45).y)])\n",
        "\n",
        "    lips = np.array([(landmarks.part(48).x, landmarks.part(48).y),\n",
        "                     (landmarks.part(49).x, landmarks.part(49).y),\n",
        "                     (landmarks.part(50).x, landmarks.part(50).y),\n",
        "                     (landmarks.part(51).x, landmarks.part(51).y),\n",
        "                     (landmarks.part(52).x, landmarks.part(52).y),\n",
        "                     (landmarks.part(53).x, landmarks.part(53).y),\n",
        "                     (landmarks.part(54).x, landmarks.part(54).y),\n",
        "                     (landmarks.part(55).x, landmarks.part(55).y),\n",
        "                     (landmarks.part(56).x, landmarks.part(56).y),\n",
        "                     (landmarks.part(57).x, landmarks.part(57).y),\n",
        "                     (landmarks.part(58).x, landmarks.part(58).y),\n",
        "                     (landmarks.part(59).x, landmarks.part(59).y)])\n",
        "\n",
        "    # Get the average color of each region\n",
        "    forehead_color = get_average_color(image, forehead)\n",
        "    nose_color = get_average_color(image, nose)\n",
        "    left_cheek_color = get_average_color(image, left_cheek)\n",
        "    right_cheek_color = get_average_color(image, right_cheek)\n",
        "    lips_color = get_average_color(image, lips)\n",
        "\n",
        "    # Print the color codes\n",
        "    print(\"Forehead Color: \", forehead_color)\n",
        "    print(\"Nose Color: \", nose_color)\n",
        "    print(\"Left Cheek Color: \", left_cheek_color)\n",
        "    print(\"Right Cheek Color: \", right_cheek_color)\n",
        "    print(\"Lips Color: \", lips_color)\n",
        "\n",
        "# Display the image with regions (optional)\n",
        "for region in [forehead, nose, left_cheek, right_cheek, lips]:\n",
        "    cv2.polylines(image, [region], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "zDQ4kd14iFmN",
        "outputId": "9192d51c-cefb-4784-f22c-1884fe7ce8bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5e98e7604cbd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image.jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the task was to convert the BGR To RGB and connect it further with openAI\n"
      ],
      "metadata": {
        "id": "TOlYitU7jIgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert BGR to RGB for analysis\n",
        "forehead_color = forehead_color[::-1]\n",
        "nose_color = nose_color[::-1]\n",
        "left_cheek_color = left_cheek_color[::-1]\n",
        "right_cheek_color = right_cheek_color[::-1]\n",
        "lips_color = lips_color[::-1]"
      ],
      "metadata": {
        "id": "VwI_lPYyjPG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now connecting with OpenAI and utilising prompt engineering to get desired results."
      ],
      "metadata": {
        "id": "zbFyK5tFjfPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# Create the prompt with the color codes\n",
        "prompt = f\"\"\"\n",
        "Analyze the following skin tone color codes and provide a color theory analysis. Determine whether the skin type is autumn, spring, winter, or summer. Additionally, suggest suitable lipstick shades for the skin tone.\n",
        "Forehead Color (RGB): {forehead_color}\n",
        "Nose Color (RGB): {nose_color}\n",
        "Left Cheek Color (RGB): {left_cheek_color}\n",
        "Right Cheek Color (RGB): {right_cheek_color}\n",
        "Lips Color (RGB): {lips_color}\n",
        "Provide a detailed analysis.\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a color theory expert.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "result=response.choices[0].message['content'].strip()"
      ],
      "metadata": {
        "id": "RGsGlFqbjpqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and creating functions for the entire task and one click solution to get desired results"
      ],
      "metadata": {
        "id": "4sX3O-RhkERw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# Load the face detector and shape predictor models\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "def get_average_color(image, region):\n",
        "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, [region], 255)\n",
        "    mean_color = cv2.mean(image, mask=mask)\n",
        "    return mean_color[:3]  # Return BGR\n",
        "\n",
        "def extract_facial_colors(image_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = detector(gray)\n",
        "    if len(faces) == 0:\n",
        "        print(\"No faces detected.\")\n",
        "        return\n",
        "\n",
        "    face = faces[0]\n",
        "    landmarks = predictor(gray, face)\n",
        "\n",
        "    # Define regions based on landmarks\n",
        "    forehead = np.array([\n",
        "        (landmarks.part(19).x, landmarks.part(19).y),\n",
        "        (landmarks.part(24).x, landmarks.part(24).y),\n",
        "        (landmarks.part(24).x, landmarks.part(24).y - 30),\n",
        "        (landmarks.part(19).x, landmarks.part(19).y - 30)\n",
        "    ])\n",
        "\n",
        "    nose = np.array([\n",
        "        (landmarks.part(27).x, landmarks.part(27).y),\n",
        "        (landmarks.part(33).x, landmarks.part(33).y),\n",
        "        (landmarks.part(31).x, landmarks.part(31).y),\n",
        "        (landmarks.part(35).x, landmarks.part(35).y)\n",
        "    ])\n",
        "\n",
        "    left_cheek = np.array([\n",
        "        (landmarks.part(2).x, landmarks.part(2).y),\n",
        "        (landmarks.part(30).x, landmarks.part(30).y),\n",
        "        (landmarks.part(28).x, landmarks.part(28).y),\n",
        "        (landmarks.part(3).x, landmarks.part(3).y)\n",
        "    ])\n",
        "\n",
        "    right_cheek = np.array([\n",
        "        (landmarks.part(14).x, landmarks.part(14).y),\n",
        "        (landmarks.part(30).x, landmarks.part(30).y),\n",
        "        (landmarks.part(28).x, landmarks.part(28).y),\n",
        "        (landmarks.part(13).x, landmarks.part(13).y)\n",
        "    ])\n",
        "\n",
        "    lips = np.array([\n",
        "        (landmarks.part(48).x, landmarks.part(48).y),\n",
        "        (landmarks.part(54).x, landmarks.part(54).y),\n",
        "        (landmarks.part(64).x, landmarks.part(64).y),\n",
        "        (landmarks.part(60).x, landmarks.part(60).y)\n",
        "    ])\n",
        "\n",
        "    # Get the average color of each region\n",
        "    forehead_color = get_average_color(image, forehead)\n",
        "    nose_color = get_average_color(image, nose)\n",
        "    left_cheek_color = get_average_color(image, left_cheek)\n",
        "    right_cheek_color = get_average_color(image, right_cheek)\n",
        "    lips_color = get_average_color(image, lips)\n",
        "\n",
        "    # Convert BGR to RGB for analysis\n",
        "    forehead_color = forehead_color[::-1]\n",
        "    nose_color = nose_color[::-1]\n",
        "    left_cheek_color = left_cheek_color[::-1]\n",
        "    right_cheek_color = right_cheek_color[::-1]\n",
        "    lips_color = lips_color[::-1]\n",
        "\n",
        "    return forehead_color, nose_color, left_cheek_color, right_cheek_color, lips_color\n",
        "\n",
        "def analyze_skin_tone_and_suggest_lipsticks(forehead_color, nose_color, left_cheek_color, right_cheek_color, lips_color):\n",
        "    # Create the prompt with the color codes\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following skin tone color codes and provide a color theory analysis. Determine whether the skin type is autumn, spring, winter, or summer. Additionally, suggest suitable lipstick shades for the skin tone.\n",
        "\n",
        "    Forehead Color (RGB): {forehead_color}\n",
        "    Nose Color (RGB): {nose_color}\n",
        "    Left Cheek Color (RGB): {left_cheek_color}\n",
        "    Right Cheek Color (RGB): {right_cheek_color}\n",
        "    Lips Color (RGB): {lips_color}\n",
        "\n",
        "    Provide a detailed analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a color theory expert.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "# Example usage\n",
        "image_path = \"image.jpeg\"\n",
        "colors = extract_facial_colors(image_path)\n",
        "if colors:\n",
        "    forehead_color, nose_color, left_cheek_color, right_cheek_color, lips_color = colors\n",
        "    analysis = analyze_skin_tone_and_suggest_lipsticks(forehead_color, nose_color, left_cheek_color, right_cheek_color, lips_color)\n",
        "    print(analysis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "collapsed": true,
        "id": "l5uvGniewvFN",
        "outputId": "b2a01e92-f0c4-4736-e4c7-38b5b7fe13e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed7f533454e2>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mforehead_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnose_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_cheek_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_cheek_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlips_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_skin_tone_and_suggest_lipsticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforehead_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnose_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_cheek_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_cheek_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlips_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ed7f533454e2>\u001b[0m in \u001b[0;36manalyze_skin_tone_and_suggest_lipsticks\u001b[0;34m(forehead_color, nose_color, left_cheek_color, right_cheek_color, lips_color)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ]
    }
  ]
}